{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\inaya\\onedrive - universitas indonesia\\term 7\\natural language processing\\nlp-lab-3-main\\env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.W = nn.Parameter(torch.Tensor(self.n_inputs, self.n_outputs))\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.1, 0.1)\n",
    "    def forward(self, x):\n",
    "        return x @ self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_width):\n",
    "        \"\"\" kernel_width harus ganjil \"\"\"\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_width = kernel_width\n",
    "        self.pad_size = (self.kernel_width - 1) // 2\n",
    "        self.kernel = Linear(kernel_width * in_channels, out_channels)\n",
    "    def forward(self, x):\n",
    "        # padding\n",
    "        x = nn.functional.pad(x, (self.pad_size, self.pad_size), \"constant\", 0)\n",
    "        l = []\n",
    "        for i in range(self.pad_size, x.shape[2] - self.pad_size):\n",
    "            patch = x[:, :, i - self.pad_size: i + self.pad_size + 1]\n",
    "            patch = patch.reshape(x.shape[0], self.in_channels * self.kernel_width)\n",
    "            l.append(self.kernel(patch))\n",
    "            \n",
    "        return torch.stack(l, dim=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6],\n",
      "         [4, 5, 6]]])\n",
      "tensor([[[1, 1],\n",
      "         [2, 2],\n",
      "         [3, 3]],\n",
      "\n",
      "        [[4, 4],\n",
      "         [5, 5],\n",
      "         [6, 6]]])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensor([[[1, 2, 3],\\n         [4, 5, 6]],\\n        [[1, 2, 3],\\n         [4, 5, 6]]])'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "print(torch.stack([x, x], dim=1))\n",
    "\"\"\" tensor([[[1, 2, 3],\n",
    "         [1, 2, 3]],\n",
    "        [[4, 5, 6],\n",
    "         [4, 5, 6]]])\n",
    "         \"\"\"\n",
    "print(torch.stack([x, x], dim=2))\n",
    "\"\"\"tensor([[[1, 1],\n",
    "         [2, 2],\n",
    "         [3, 3]],\n",
    "        [[4, 4],\n",
    "         [5, 5],\n",
    "         [6, 6]]])\"\"\"\n",
    "print(torch.stack([x, x], dim=0))\n",
    "\n",
    "\"\"\"tensor([[[1, 2, 3],\n",
    "         [4, 5, 6]],\n",
    "        [[1, 2, 3],\n",
    "         [4, 5, 6]]])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 5])\n",
      "tensor([[[ 0.5090,  0.2823,  0.2535,  0.2247,  0.1547],\n",
      "         [-0.2372, -0.1277, -0.3058, -0.4839, -0.7776],\n",
      "         [ 0.2449,  0.4284,  0.1594, -0.1096, -0.0069],\n",
      "         [ 0.1822,  0.0292,  0.1593,  0.2893,  0.0346],\n",
      "         [-0.3418, -0.3281, -0.0197,  0.2888,  0.3753],\n",
      "         [ 0.2777,  0.4209,  0.2498,  0.0786, -0.1206],\n",
      "         [ 0.1046, -0.1358, -0.2450, -0.3543,  0.0033]],\n",
      "\n",
      "        [[ 0.5090,  0.2823,  0.2535,  0.2247,  0.1547],\n",
      "         [-0.2372, -0.1277, -0.3058, -0.4839, -0.7776],\n",
      "         [ 0.2449,  0.4284,  0.1594, -0.1096, -0.0069],\n",
      "         [ 0.1822,  0.0292,  0.1593,  0.2893,  0.0346],\n",
      "         [-0.3418, -0.3281, -0.0197,  0.2888,  0.3753],\n",
      "         [ 0.2777,  0.4209,  0.2498,  0.0786, -0.1206],\n",
      "         [ 0.1046, -0.1358, -0.2450, -0.3543,  0.0033]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq = torch.tensor([[[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]],\n",
    "[[1.,2.,3.,4.,5.], [5.,4.,3.,2.,1.]]])\n",
    "\n",
    "print(seq.shape) \n",
    "\n",
    "conv = Conv1D(2, 7, 3)\n",
    " # torch.size([2, 2, 5])\n",
    "print(conv(seq).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m conv \u001b[38;5;241m=\u001b[39m Conv1D(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m  \u001b[38;5;66;03m# torch.size([2, 2, 5])\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape) \n",
      "File \u001b[1;32mc:\\Users\\inaya\\OneDrive - UNIVERSITAS INDONESIA\\TERM 7\\Natural Language Processing\\NLP-Lab-3-main\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\inaya\\OneDrive - UNIVERSITAS INDONESIA\\TERM 7\\Natural Language Processing\\NLP-Lab-3-main\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m l \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size, \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size):\n\u001b[0;32m     15\u001b[0m     patch \u001b[38;5;241m=\u001b[39m x[:, :, i \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m     patch \u001b[38;5;241m=\u001b[39m patch\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_width)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "seq = torch.tensor([[1,2,3,],[4,5,6]])\n",
    "\n",
    "# print(seq.shape) \n",
    "\n",
    "conv = Conv1D(2, 2, 3)\n",
    " # torch.size([2, 2, 5])\n",
    "print(conv(seq).shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
